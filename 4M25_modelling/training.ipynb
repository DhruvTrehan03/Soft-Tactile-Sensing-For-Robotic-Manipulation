{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from torch import nn\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard to plot losses\n",
    "\n",
    "from dataset_creator import EITDataset\n",
    "from network_models import EITNet\n",
    "\n",
    "# configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use GPU if available\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: /home/asimov/Soft-Tactile-Sensing-For-Robotic-Manipulation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the working directory to the script's directory\n",
    "script_dir = os.path.dirname('/home/asimov/Soft-Tactile-Sensing-For-Robotic-Manipulation/')\n",
    "os.chdir(script_dir)\n",
    "print(\"Changed working directory to:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 608\n",
      "Validation dataset size: 151\n"
     ]
    }
   ],
   "source": [
    "## Configure dataset and dataloader\n",
    "data_folder = \"Readings/Funky/\"\n",
    "\n",
    "# find data_folder\n",
    "if not os.path.exists(data_folder):\n",
    "    raise Exception(f\"Data folder {data_folder} does not exist\")\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = EITDataset(data_folder)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "generator1 = torch.Generator().manual_seed(91) # rng seed for splitting into train and val\n",
    "split = random_split(dataset, [0.8, 0.2], generator=generator1) # split dataset into 80% train and 20% val\n",
    "\n",
    "train_dataset = split[0] # get training dataset\n",
    "val_dataset = split[1] # get validation dataset\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True) # create dataloader for training dataset\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 16, shuffle = True) # create dataloader for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 1 sample\n",
    "sample = dataset[1]\n",
    "readings = sample[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EITNet(\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (shape_head): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (pos_head): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (orient_head): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Initialise network\n",
    "# Define network\n",
    "model = EITNet().to(device) # move model to device\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training 1 epoch\n",
    "\n",
    "def train_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_epoch_loss = 0.0\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        readings, labels = data\n",
    "        readings = readings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        shape_labels = labels[:, 0]\n",
    "        position_labels = labels[:, 1]\n",
    "        orientation_labels = labels[:, 2]\n",
    "        \n",
    "        # Zero your gradients for every batch\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        shape_logits, position_logits, orientation_logits = model(readings)\n",
    "        \n",
    "        \n",
    "        # Compute the total loss and its gradients\n",
    "        # loss for each head\n",
    "        loss_shape = criterion(shape_logits, shape_labels)\n",
    "        loss_position = criterion(position_logits, position_labels)\n",
    "        loss_orientation = criterion(orientation_logits, orientation_labels)\n",
    "        \n",
    "        # total loss\n",
    "        loss = loss_shape + loss_position + loss_orientation\n",
    "        \n",
    "        loss.backward() # grads\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Gather data and report to TensorBoard every 10 batches\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') # timestamp for TensorBoard logs\n",
    "writer = SummaryWriter('4M25_modelling/runs/model_test_{}'.format(timestamp)) # create TensorBoard writer for each run\n",
    "epoch_number = 0 # initialise epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 3.2618546009063722\n",
      "  batch 20 loss: 2.7697469711303713\n",
      "  batch 30 loss: 2.477886438369751\n",
      "LOSS train 2.477886438369751 valid 1.8569942712783813\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 1.9621262788772582\n",
      "  batch 20 loss: 1.812419593334198\n",
      "  batch 30 loss: 1.8162100315093994\n",
      "LOSS train 1.8162100315093994 valid 1.4684627056121826\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 1.611629021167755\n",
      "  batch 20 loss: 1.4031672835350038\n",
      "  batch 30 loss: 1.37778902053833\n",
      "LOSS train 1.37778902053833 valid 1.2033379077911377\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 1.2318739652633668\n",
      "  batch 20 loss: 1.202217435836792\n",
      "  batch 30 loss: 1.1935820817947387\n",
      "LOSS train 1.1935820817947387 valid 0.9958569407463074\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.9864594399929046\n",
      "  batch 20 loss: 0.9858055531978607\n",
      "  batch 30 loss: 0.8946448504924774\n",
      "LOSS train 0.8946448504924774 valid 0.9048153162002563\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.8120444774627685\n",
      "  batch 20 loss: 0.7788848280906677\n",
      "  batch 30 loss: 0.8067529797554016\n",
      "LOSS train 0.8067529797554016 valid 0.8061431646347046\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.7015457630157471\n",
      "  batch 20 loss: 0.7481839656829834\n",
      "  batch 30 loss: 0.6910423040390015\n",
      "LOSS train 0.6910423040390015 valid 0.7053197622299194\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.5952318727970123\n",
      "  batch 20 loss: 0.6863604962825776\n",
      "  batch 30 loss: 0.6581956148147583\n",
      "LOSS train 0.6581956148147583 valid 0.6970812678337097\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.5333797633647919\n",
      "  batch 20 loss: 0.5954475224018096\n",
      "  batch 30 loss: 0.506049194931984\n",
      "LOSS train 0.506049194931984 valid 0.6598331332206726\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.4273498386144638\n",
      "  batch 20 loss: 0.4476695716381073\n",
      "  batch 30 loss: 0.4506594479084015\n",
      "LOSS train 0.4506594479084015 valid 0.6440848708152771\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.42917143255472184\n",
      "  batch 20 loss: 0.48082154989242554\n",
      "  batch 30 loss: 0.4889031708240509\n",
      "LOSS train 0.4889031708240509 valid 0.5844295620918274\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.4220273599028587\n",
      "  batch 20 loss: 0.355048967897892\n",
      "  batch 30 loss: 0.3886179715394974\n",
      "LOSS train 0.3886179715394974 valid 0.5990990400314331\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.34056801348924637\n",
      "  batch 20 loss: 0.33479428440332415\n",
      "  batch 30 loss: 0.34350280463695526\n",
      "LOSS train 0.34350280463695526 valid 0.5527433156967163\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.3512236297130585\n",
      "  batch 20 loss: 0.3160557448863983\n",
      "  batch 30 loss: 0.2904612869024277\n",
      "LOSS train 0.2904612869024277 valid 0.5324109792709351\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.35426114648580553\n",
      "  batch 20 loss: 0.2708206780254841\n",
      "  batch 30 loss: 0.34429805874824526\n",
      "LOSS train 0.34429805874824526 valid 0.5531060099601746\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.32180059552192686\n",
      "  batch 20 loss: 0.27330331802368163\n",
      "  batch 30 loss: 0.3412740260362625\n",
      "LOSS train 0.3412740260362625 valid 0.5720607042312622\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.30479119569063184\n",
      "  batch 20 loss: 0.25841031819581983\n",
      "  batch 30 loss: 0.3157813727855682\n",
      "LOSS train 0.3157813727855682 valid 0.5895450115203857\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.24928152188658714\n",
      "  batch 20 loss: 0.2977199375629425\n",
      "  batch 30 loss: 0.28464706987142563\n",
      "LOSS train 0.28464706987142563 valid 0.5810542106628418\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.24106677174568175\n",
      "  batch 20 loss: 0.1975874625146389\n",
      "  batch 30 loss: 0.276494225859642\n",
      "LOSS train 0.276494225859642 valid 0.5660918951034546\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.26457012444734573\n",
      "  batch 20 loss: 0.31132242679595945\n",
      "  batch 30 loss: 0.18601511269807816\n",
      "LOSS train 0.18601511269807816 valid 0.5974808931350708\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.18935107588768005\n",
      "  batch 20 loss: 0.2478497475385666\n",
      "  batch 30 loss: 0.24425295293331145\n",
      "LOSS train 0.24425295293331145 valid 0.6201143264770508\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.25062238946557047\n",
      "  batch 20 loss: 0.17728756740689278\n",
      "  batch 30 loss: 0.33802133798599243\n",
      "LOSS train 0.33802133798599243 valid 0.5976814031600952\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.21934080496430397\n",
      "  batch 20 loss: 0.220170047134161\n",
      "  batch 30 loss: 0.2517150014638901\n",
      "LOSS train 0.2517150014638901 valid 0.6301194429397583\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.2802749782800674\n",
      "  batch 20 loss: 0.19119271039962768\n",
      "  batch 30 loss: 0.2722143642604351\n",
      "LOSS train 0.2722143642604351 valid 0.5702263712882996\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.31537762358784677\n",
      "  batch 20 loss: 0.30567475482821466\n",
      "  batch 30 loss: 0.2757453188300133\n",
      "LOSS train 0.2757453188300133 valid 0.6865526437759399\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.20489277616143226\n",
      "  batch 20 loss: 0.2293379172682762\n",
      "  batch 30 loss: 0.15331526845693588\n",
      "LOSS train 0.15331526845693588 valid 0.5827533006668091\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.31624885648489\n",
      "  batch 20 loss: 0.13115347921848297\n",
      "  batch 30 loss: 0.19633524045348166\n",
      "LOSS train 0.19633524045348166 valid 0.6498352289199829\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.23535819686949253\n",
      "  batch 20 loss: 0.2241688773036003\n",
      "  batch 30 loss: 0.19917408302426337\n",
      "LOSS train 0.19917408302426337 valid 0.6627980470657349\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.17804848812520505\n",
      "  batch 20 loss: 0.18115101084113122\n",
      "  batch 30 loss: 0.12569178491830826\n",
      "LOSS train 0.12569178491830826 valid 0.590462327003479\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.10919587165117264\n",
      "  batch 20 loss: 0.1857248030602932\n",
      "  batch 30 loss: 0.2650533847510815\n",
      "LOSS train 0.2650533847510815 valid 0.5707135796546936\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.14769225791096688\n",
      "  batch 20 loss: 0.1623230952769518\n",
      "  batch 30 loss: 0.23543055094778537\n",
      "LOSS train 0.23543055094778537 valid 0.6537514925003052\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.13877929262816907\n",
      "  batch 20 loss: 0.18036523684859276\n",
      "  batch 30 loss: 0.18999959025532007\n",
      "LOSS train 0.18999959025532007 valid 0.6263015866279602\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.15799493938684464\n",
      "  batch 20 loss: 0.2278137419372797\n",
      "  batch 30 loss: 0.17499069459736347\n",
      "LOSS train 0.17499069459736347 valid 0.5701364874839783\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.2289399467408657\n",
      "  batch 20 loss: 0.16727107390761375\n",
      "  batch 30 loss: 0.1268061313778162\n",
      "LOSS train 0.1268061313778162 valid 0.5881472229957581\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.1291238058358431\n",
      "  batch 20 loss: 0.17432187795639037\n",
      "  batch 30 loss: 0.15420207865536212\n",
      "LOSS train 0.15420207865536212 valid 0.6323044896125793\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.10291877612471581\n",
      "  batch 20 loss: 0.11258210018277168\n",
      "  batch 30 loss: 0.12007531151175499\n",
      "LOSS train 0.12007531151175499 valid 0.5430586934089661\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.1890130378305912\n",
      "  batch 20 loss: 0.12357394210994244\n",
      "  batch 30 loss: 0.27121649459004404\n",
      "LOSS train 0.27121649459004404 valid 0.6618363857269287\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.13449923023581506\n",
      "  batch 20 loss: 0.09617005325853825\n",
      "  batch 30 loss: 0.1149040974676609\n",
      "LOSS train 0.1149040974676609 valid 0.6065423488616943\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.1215162631124258\n",
      "  batch 20 loss: 0.11924779117107391\n",
      "  batch 30 loss: 0.098204081133008\n",
      "LOSS train 0.098204081133008 valid 0.6275661587715149\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.14529537297785283\n",
      "  batch 20 loss: 0.15823738202452658\n",
      "  batch 30 loss: 0.14877311028540136\n",
      "LOSS train 0.14877311028540136 valid 0.6213306188583374\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.0949825088493526\n",
      "  batch 20 loss: 0.11221042908728122\n",
      "  batch 30 loss: 0.10779156237840652\n",
      "LOSS train 0.10779156237840652 valid 0.6349948644638062\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.2267665009945631\n",
      "  batch 20 loss: 0.07074319198727608\n",
      "  batch 30 loss: 0.22485699728131295\n",
      "LOSS train 0.22485699728131295 valid 0.5733445286750793\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.15211785435676575\n",
      "  batch 20 loss: 0.22545187138020992\n",
      "  batch 30 loss: 0.1904901687055826\n",
      "LOSS train 0.1904901687055826 valid 0.6686823964118958\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.11808961015194655\n",
      "  batch 20 loss: 0.22856959216296674\n",
      "  batch 30 loss: 0.12452927865087986\n",
      "LOSS train 0.12452927865087986 valid 0.7378972172737122\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.13098327182233332\n",
      "  batch 20 loss: 0.18312640115618706\n",
      "  batch 30 loss: 0.16653089188039302\n",
      "LOSS train 0.16653089188039302 valid 0.6436400413513184\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.11456321440637111\n",
      "  batch 20 loss: 0.15509016774594783\n",
      "  batch 30 loss: 0.1613379156216979\n",
      "LOSS train 0.1613379156216979 valid 0.7714937329292297\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.1987926498055458\n",
      "  batch 20 loss: 0.0945208614692092\n",
      "  batch 30 loss: 0.12250434029847383\n",
      "LOSS train 0.12250434029847383 valid 0.6463767290115356\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.09623660370707512\n",
      "  batch 20 loss: 0.12369781620800495\n",
      "  batch 30 loss: 0.16986981369554996\n",
      "LOSS train 0.16986981369554996 valid 0.5857502222061157\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.14352967999875546\n",
      "  batch 20 loss: 0.0724402166903019\n",
      "  batch 30 loss: 0.09709717854857444\n",
      "LOSS train 0.09709717854857444 valid 0.633693516254425\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.11723414286971093\n",
      "  batch 20 loss: 0.10934513919055462\n",
      "  batch 30 loss: 0.10113002508878707\n",
      "LOSS train 0.10113002508878707 valid 0.589281439781189\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "EPOCHS = 50 # number of epochs\n",
    "\n",
    "best_vloss = 1_000_000. # initialise best validation loss to a large value\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_epoch(epoch_number, writer)\n",
    "    \n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(val_dataloader):\n",
    "        vreadings, vlabels = vdata\n",
    "        vreadings = vreadings.to(device)\n",
    "        vlabels = vlabels.to(device)\n",
    "        vshape_labels = vlabels[:, 0]\n",
    "        vposition_labels = vlabels[:, 1]\n",
    "        vorientation_labels = vlabels[:, 2]\n",
    "        \n",
    "        vshape_logits, vposition_logits, vorientation_logits = model(vreadings)\n",
    "        \n",
    "        vloss_shape = criterion(vshape_logits, vshape_labels)\n",
    "        vloss_position = criterion(vposition_logits, vposition_labels)\n",
    "        vloss_orientation = criterion(vorientation_logits, vorientation_labels)\n",
    "        \n",
    "        # total loss\n",
    "        vloss = vloss_shape + vloss_position + vloss_orientation\n",
    "        \n",
    "        running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = '4M25_modelling/trained_models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from 4M25_modelling/trained_models/model_20250317_162245_13\n",
      "Accuracy of the network on the validation set: 78 %\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('Loaded model from', model_path)\n",
    "model.eval()\n",
    "\n",
    "# Test the model on the validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_dataloader:\n",
    "        readings, labels = data\n",
    "        readings = readings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        shape_labels = labels[:, 0]\n",
    "        position_labels = labels[:, 1]\n",
    "        orientation_labels = labels[:, 2]\n",
    "        \n",
    "        shape_logits, position_logits, orientation_logits = model(readings)\n",
    "        \n",
    "        _, shape_pred = torch.max(shape_logits, 1)\n",
    "        _, position_pred = torch.max(position_logits, 1)\n",
    "        _, orientation_pred = torch.max(orientation_logits, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += ((shape_pred == shape_labels) & (position_pred == position_labels) & (orientation_pred == orientation_labels)).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": ".conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
